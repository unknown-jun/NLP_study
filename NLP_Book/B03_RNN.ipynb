{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "B03_RNN",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXEjiuO2pPE/FSsfGvoCIo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unknown-jun/NLP_study/blob/main/NLP_Book/B03_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# RNN 파이썬 구현"
      ],
      "metadata": {
        "id": "bSzuBarsVROy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "timesteps = 10    # 시점의 수. NLP에서는 보통 문장의 길이가 된다.\n",
        "input_size = 4    # 입력 차원. NLP에서는 보통 단어 벡터의 차원이 된다.\n",
        "hidden_size = 8   # 은닉 상태의 크기. 메모리셀의 용량이다.\n",
        "\n",
        "# 입력에 해당되는 2D 텐서\n",
        "inputs = np.random.random((timesteps, input_size))\n",
        "\n",
        "# 은닉상태의 크기 hidden_size로 은닉 상태를 만듬\n",
        "hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태는 0(벡터)로 초기화"
      ],
      "metadata": {
        "id": "AdOXKVVyoM4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_state_t  #8의 크기를 가지는 은닉상태. 현재는 초기 은닉 상태로 모든 차원이 0을 가짐"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ClFbsIkpbzN",
        "outputId": "c20a6caa-7e56-4c6b-87f9-2ef0525803a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Wx = np.random.random((hidden_size, input_size))    # (8,4) 크기의 2D 텐서 생성. 입력에 대한 가중치\n",
        "Wh = np.random.random((hidden_size, hidden_size))   # (8,8) 크기의 2D 텐서 생성. 은닉상태에 대한 가중치\n",
        "b = np.random.random((hidden_size,))                # (8,) 크기의 1D 텐서 생성. 이 값은 편향(bias)\n",
        "\n",
        "print(np.shape(Wx))\n",
        "print(np.shape(Wh))\n",
        "print(np.shape(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6BFoGQ5pk93",
        "outputId": "947a7866-a3db-4db1-a517-9f87c040c646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 4)\n",
            "(8, 8)\n",
            "(8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_hidden_state = []\n",
        "\n",
        "# 메모리 셀 동작\n",
        "for input_t in inputs:\n",
        "  output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b) # Wx * Xt + Wh * Ht-1 + bias\n",
        "  total_hidden_state.append(output_t)  # 각 시점의 은닉상태의 값을 계속해서 축적\n",
        "  hidden_state_t = output_t\n",
        "\n",
        "total_hidden_state = np.stack(total_hidden_state, axis=0)\n",
        "total_hidden_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFzYp4dDqL2i",
        "outputId": "55577751-32d1-4d4b-d6e8-b246c257f0db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99998369, 0.99975207, 0.99999221, 0.99923166, 0.99989322,\n",
              "        0.99998675, 0.99995146, 0.99995514],\n",
              "       [0.99994201, 0.99978881, 0.99999077, 0.99811641, 0.99994927,\n",
              "        0.99998456, 0.9998886 , 0.9999048 ],\n",
              "       [0.9999364 , 0.9996629 , 0.99997297, 0.99835878, 0.9998659 ,\n",
              "        0.99995695, 0.99986765, 0.9999249 ],\n",
              "       [0.99996672, 0.9998319 , 0.9999953 , 0.99867714, 0.9999604 ,\n",
              "        0.99999167, 0.99993234, 0.99992524],\n",
              "       [0.99996449, 0.99986346, 0.99999604, 0.99907703, 0.99996439,\n",
              "        0.99999178, 0.99994428, 0.99995282],\n",
              "       [0.99994476, 0.99984901, 0.99999401, 0.99864358, 0.99996485,\n",
              "        0.99998851, 0.99993007, 0.9999289 ],\n",
              "       [0.99994691, 0.99983931, 0.99999199, 0.99906522, 0.9999486 ,\n",
              "        0.99998329, 0.99993855, 0.99995538],\n",
              "       [0.99990642, 0.99979719, 0.99998575, 0.99833588, 0.99994351,\n",
              "        0.99997363, 0.99989001, 0.99992455],\n",
              "       [0.99998136, 0.99994319, 0.99999896, 0.99971784, 0.99998401,\n",
              "        0.99999689, 0.99998863, 0.99998272],\n",
              "       [0.99994465, 0.99984347, 0.99999296, 0.99906183, 0.9999516 ,\n",
              "        0.99998478, 0.99992402, 0.99995961]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_size = 5   # 입력의 크기\n",
        "hidden_size = 8  # 은닉 상태의 크기\n",
        "\n",
        "# (batch_size, time_steps, input_size)\n",
        "inputs = torch.Tensor(1, 10, 5)\n",
        "\n",
        "cell = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "outputs, _status = cell(inputs)\n",
        "print(outputs.shape)  # 모든 시점(timesteps)의 은닉 상태들\n",
        "print(_status.shape)  # 마지막 시점(timestep)의 은닉 상태"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mirnLnZF6wBI",
        "outputId": "c80990e9-75a7-4bfb-a73f-1b948bc5613b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 8])\n",
            "torch.Size([1, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class RNN:\n",
        "  def __init__(self, Wx, Wh, b):\n",
        "    self.params = [Wx, Wh, b]  \n",
        "    # 가중치 2개와 편향 1개를 인수로 받음\n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)] \n",
        "    # 각 매개변수에 대응하는 형태로 기울기를 초기화한 후 grads에 저장\n",
        "    self.cache = None\n",
        "    # 역전파 계산 시 사용하는 중간 데이터를 담을 cache를 초기화\n",
        "\n",
        "  def forward(self, x, h_prev):\n",
        "    Wx, Wh, b = self.params\n",
        "    t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
        "    h_next = np.tanh(t)\n",
        "\n",
        "    self.cache = (x, h_prev, h_next)\n",
        "    return h_next\n",
        "\n",
        "  def backward(self, dh_next):\n",
        "    Wx, Wh, b = self.params\n",
        "    x, h_prev, h_next = self.cache\n",
        "\n",
        "    dt = dh_next * (1 - h_next ** 2)\n",
        "    db = np.sum(dt, axis=0)\n",
        "    dWh = np.dot(h_prev.T, dt)\n",
        "    dh_prev = np.dot(dt, Wh.T)\n",
        "    dWx = np.dot(x.T, dt)\n",
        "    dx = np.dot(dt, Wx.T)\n",
        "\n",
        "    self.grads[0][...] = dWx\n",
        "    self.grads[1][...] = dWh\n",
        "    self.grads[2][...] = db\n",
        "\n",
        "    return dx, dh_prev"
      ],
      "metadata": {
        "id": "EM_Vi9t0VR4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeRNN:\n",
        "  # 초기화 메소드\n",
        "  def __init__(self, Wx, Wh, b, stateful=False): \n",
        "    # 가중치, 편향, 은닉상태 인계여부(실제에서는 True)\n",
        "    self.params = [Wx, Wh, b]\n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "    self.layers = None # 다수의 RNN 계층을 리스트로 저장\n",
        "\n",
        "    # h: 마지막 RNN 계층의 은닉상태 저장\n",
        "    # dh: 역전파에서 하나 앞 블록의 은닉상태 기울기 저장\n",
        "    self.h, self.dh = None, None\n",
        "    self.stateful = stateful # 은닉상태 유지 True\n",
        "    # stateful = True: Time RNN 계층이 은닉상태를 유지한다.\n",
        "    # -> 아무리 긴 데이터라도 Time RNN 계층의 순전파를 끊지 않고 전파한다.\n",
        "    # stateful = False: Time RNN 계층은 은닉상태를 '영행렬'로 초기화한다. 상태가 없다.\n",
        "\n",
        "  def set_state(self, h):\n",
        "    self.h = h\n",
        "\n",
        "  def reset_state(self, h):\n",
        "    self.h = None\n",
        "\n",
        "  # 순전파\n",
        "  def forward(self, xs):\n",
        "    Wx, Wh, b = self.params\n",
        "    # xs.shape = (N, T, D) = (미니배치 크기, T개 분량 시계열 데이터, 입력벡터 차원 수)\n",
        "    N, T, D = xs.shape \n",
        "    D, H = Wx.shape\n",
        "\n",
        "    self.layers = []\n",
        "    hs = np.empty((N, T, H), dtype='f') # 출력값을 담은 hs\n",
        "\n",
        "    if not self.stateful or self.h is None:\n",
        "      self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "    for t in range(T):\n",
        "      layer = RNN(*self.params)\n",
        "      self.h = layer.forward(xs[:, t, :], self.h)\n",
        "      hs[:,t,:] = self.h\n",
        "      self.layers.append(layer)\n",
        "\n",
        "    return hs\n",
        "\n",
        "  def backward(self, dhs):\n",
        "    Wx, Wh, b = self.params\n",
        "    N, T, D = dhs.shape\n",
        "    D, H = Wx.shape\n",
        "\n",
        "    dxs = np.empty((N, T, D), dtype='f')\n",
        "    dh = 0\n",
        "    grads = [0,0,0]\n",
        "\n",
        "    for t in reversed(range(T)):\n",
        "      layer = self.layers[t]\n",
        "      dx, dh = layer.backward(dhs[:, t, :] + dh) # 합산된 기울기\n",
        "      dxs[:, t, :] = dx\n",
        "\n",
        "      for i, grad in enumerate(layer.grads):\n",
        "        grads[i] += grad\n",
        "\n",
        "    for i, grad in enumerate(grads):\n",
        "      self.grad[i][...] = grad\n",
        "    self.dh = dh\n",
        "\n",
        "    return dxs"
      ],
      "metadata": {
        "id": "EWXfO3LLVbiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN 셀 구현"
      ],
      "metadata": {
        "id": "TSvKp3x7y_f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZyoNhy0n96B",
        "outputId": "4f45a29d-af05-4627-b9f4-ac218b75dbe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhgNUd71XNRx"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 로드\n",
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "start = time.time()\n",
        "TEXT = torchtext.legacy.data.Field(lower=True, fix_length=200, batch_first=False)\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False)"
      ],
      "metadata": {
        "id": "c0QfW8hZZUlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy import datasets\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "metadata": {
        "id": "rRdTS7a9anYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02bb1404-3d78-4459-a344-011d1bf4eb5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:02<00:00, 31.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.text.legacy.data.Field**  \n",
        "데이터 전처리를 위해 사용된다.\n",
        "1. fix_length: 고정된 길이의 데이터를 얻을 수 있다.  \n",
        "여기에서는 데이터의 길이를 200으로 고정했으며 200보다 짧다면 패딩 작업을 통해 200으로 맞추어 줌\n",
        "2. batch_first: 신경망에 입력되는 텐서의 첫 번째 차원의 값이 배치 크기가 되도록 함.  \n",
        "기본 값은 False.  \n",
        "모델의 네트워크로 입력되는 데이터는 [시퀀스 길이, 배치 크기, 은닉층의 뉴런 개수]의 형태이지만 batch_first=True로 설정하면 [배치크기, 시퀀스의 길이, 은닉층의 뉴런 개수] 형태로 변경된다.\n",
        "3. sequential: 데이터에 순서가 있는지 나타내며 기본값은 True. 예제의 레이블은 긍정/부정 값만 갖기 때문에 False로 설정"
      ],
      "metadata": {
        "id": "w0JGACK_ZqRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 전처리 적용\n",
        "import string\n",
        "\n",
        "for example in train_data.examples:\n",
        "  text = [x.lower() for x in vars(example)['text']]  # 소문자로 변경\n",
        "  text = [x.replace(\"<br\",\"\") for x in text]         # \"<br\"을 공백으로 변경\n",
        "  text = [''.join(c for c in s if c not in string.punctuation) for s in text] # 구두점 제거\n",
        "  text = [s for s in text if s]  # 공백 제거\n",
        "  vars(example)['text'] = text"
      ],
      "metadata": {
        "id": "5i5h3i36aum0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련과 검증 데이터셋 분리\n",
        "import random\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(0), \n",
        "                                          split_ratio=0.8)\n",
        "\n",
        "print(f'Number of training examples:   {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples:    {len(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-ekqLTnbnXC",
        "outputId": "f81197dd-72e8-4c03-d509-2b38c69fa9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples:   20000\n",
            "Number of validation examples: 5000\n",
            "Number of testing examples:    25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 집합 만들기\n",
        "\n",
        "'''\n",
        "단어 집합이란 IMDB 데이터셋에 포함된 단어를 이용하여 하나의 딕셔너리와 같은 집합을 만드는 것\n",
        "단어 집합을 만들 때는 단어들의 중복은 제거된 상태에서 진행함\n",
        "'''\n",
        "TEXT.build_vocab(train_data, max_size=10000, min_freq=10,vectors=None)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'Number of TEXT vocabulary:  {len(TEXT.vocab)}')\n",
        "print(f'Number of LABEL vocabulary: {len(LABEL.vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIzMhTWkcI3W",
        "outputId": "743b6449-33df-4e55-fd60-9b2c2691897f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of TEXT vocabulary:  10002\n",
            "Number of LABEL vocabulary: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**build_vocab**  \n",
        "1. max_size: 단어 집합으 크기로 단어 집합에 포함되는 어휘 수를 의미함\n",
        "2. min_freq: 훈련 데이터셋에서 특정 단어의 최소 등장 횟수를 의미함.  \n",
        "즉. min_freq= 10으로 설정했기 때문에 훈련 데이터셋에서 특정 단어가 최소 열번 이상 등장한 것만 단어 집합에 추가하겠다는 의미\n",
        "3. vectors: 임베딩 벡터를 지정할 수 있다.  \n",
        "임베딩 벡터는 워드 임베딩의 결과로 나온 벡터. 사전 학습된 임베딩으로는 워드투벡터(Word 2 Vector), 글로브(Glove) 등이 있으며, 파이토치에서도 nn.embedding()을 통해 단어를 랜덤한 숫자 값으로 변환한 후 가중치를 학습하는 방법을 제공함"
      ],
      "metadata": {
        "id": "eImI1Ex3dN9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터셋의 단어 집합 확인\n",
        "print(LABEL.vocab.stoi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMQmQaYBcxw6",
        "outputId": "90190e62-0717-4748-eab2-f088c160d729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<bound method Vocab._default_unk_index of <torchtext.legacy.vocab.Vocab object at 0x7f92d0fe7410>>, {'<unk>': 0, 'pos': 1, 'neg': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 메모리로 가져오기\n",
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "embeding_dim = 100  # 각 단어를 100차원으로 조정(임베딩 계층을 통과한 후 각 벡터의 크기)\n",
        "hidden_size = 300\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size= BATCH_SIZE,\n",
        "    device= device\n",
        ")"
      ],
      "metadata": {
        "id": "zIel9WTdd19C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BucketIterator**  \n",
        "데이터로더(dataloader)와 쓰임새가 같다.  \n",
        "즉, 배치 크기 단위로 값을 차례로 꺼내어 메모리로 가져오고 싶을 때 사용함. 특히 Field에서 fix_length를 사용하지 않았다면 BucketIterator에서 데이터의 길이를 조정할 수 있다. BucketIterator는 비슷한 길이의 데이터를 한 배치에 할당하여 패딩(padding)을 최소화시켜 줌"
      ],
      "metadata": {
        "id": "JAVR_VCBgS1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 워드 임베딩 및 RNN 셀 정의\n",
        "class RNNCell_Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_size):\n",
        "    super(RNNCell_Encoder, self).__init__()\n",
        "    self.rnn = nn.RNNCell(input_dim, hidden_size)\n",
        "\n",
        "  def forward(self, inputs):     # inputs는 입력 시퀀스로 (시퀀스 길이, 배치, 임베딩(seq, batch, embedding))의 형태를 가짐\n",
        "    bz = inputs.shape[1]         # 배치 차원을 가져옴\n",
        "    ht = torch.zeros((bz, hidden_size)).to(device)  # 배치와 은닉층 뉴런의 크기를 0으로 초기화\n",
        "    for word in inputs:\n",
        "      ht = self.rnn(word, ht)\n",
        "    return ht\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.em = nn.Embedding(len(TEXT.vocab.stoi), embeding_dim)\n",
        "    self.rnn = RNNCell_Encoder(embeding_dim, hidden_size)\n",
        "    self.fc1 = nn.Linear(hidden_size, 256)\n",
        "    self.fc2 = nn.Linear(256,3)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.em(x)\n",
        "    x = self.rnn(x)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "dmF9XmJQedRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 옵티마이저와 손실 함수 정의\n",
        "model = Net()\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "-GG8uK8cihdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.nn.CrossEntropyLoss()**  \n",
        "다중 분류에 사용됨.  \n",
        "torch.nn.CrossEntropyLoss는 nn.LogSoftmax와 nn.NLLLoss 연산의 조합으로 구성.  \n",
        "nn.LogSoftmax는 모델 네트워크의 마지막 계층에서 얻은 결괏값들을 확률로 해석하기 위해 소프트맥스 함수의 결과에 로그(log)를 취한 것이고, nn.NLLLoss는 다중분류에 사용된다. 신경망에서 로그 확률 값을 얻으려면 마지막에 LogSoftmax를 추가해야 함"
      ],
      "metadata": {
        "id": "fTdvnsiasnfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습을 위한 함수 정의\n",
        "# 1. 데이터 로더에서 데이터를 가져와서 \n",
        "# 2. 모델에 적용한 후 \n",
        "# 3. 손실 함수를 적용하여 오차를 구하고\n",
        "# 4. 옵티마이저를 이용하여 파라미터(가중치, 바이어스 등)를 업데이트\n",
        "\n",
        "def training(epoch, model, trainloader, validloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  running_loss = 0\n",
        "\n",
        "  model.train()\n",
        "  for b in trainloader:\n",
        "    x, y = b.text, b.label             # trainloader에서 text와 label을 꺼내 옴\n",
        "    x, y = x.to(device), y.to(device)  # 꺼내 온 데이터를 gpu로 옮김\n",
        "\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred, y)          # CrossEntropyLoss 손실 함수를 이용하여 오차 계산\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      y_pred = torch.argmax(y_pred, dim=1)\n",
        "      correct += (y_pred == y).sum().item()\n",
        "      total += y.size(0)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "  epoch_loss = running_loss / len(trainloader.dataset)   # 누적된 오차를 전체 데이터셋으로 나누어서 에포크 단계마다 오차를 구함\n",
        "  epoch_acc = correct / total\n",
        "\n",
        "  valid_correct = 0\n",
        "  valid_total = 0\n",
        "  valid_running_loss = 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for b in validloader:\n",
        "      x, y = b.text, b.label\n",
        "      x, y = x.to(device), y.to(device)\n",
        "\n",
        "      y_pred = model(x)\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      \n",
        "      y_pred = torch.argmax(y_pred, dim=1)\n",
        "      valid_correct += (y_pred == y).sum().item()\n",
        "      valid_total += y.size(0)\n",
        "      valid_running_loss += loss.item()\n",
        "\n",
        "  epoch_valid_loss = valid_running_loss / len(validloader.dataset)\n",
        "  epoch_valid_acc = valid_correct / valid_total\n",
        "\n",
        "  # 훈련이 진행될 때 에포크마다 정확도와 오차(loss)를 출력\n",
        "  print('epoch: ', epoch,\n",
        "        'loss: ', round(epoch_loss, 3),\n",
        "        'accuracy: ', round(epoch_acc, 3),\n",
        "        'valid_loss: ', round(epoch_valid_loss, 3),\n",
        "        'valid_accuracy: ', round(epoch_valid_acc, 3))\n",
        "  \n",
        "  return epoch_loss, epoch_acc, epoch_valid_loss, epoch_valid_acc"
      ],
      "metadata": {
        "id": "foC1MusWsgRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "epochs = 5\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "valid_loss = []\n",
        "valid_acc = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss, epoch_acc, epoch_valid_loss, epoch_valid_acc = training(epoch, model, train_iterator, valid_iterator)\n",
        "  train_loss.append(epoch_loss)\n",
        "  train_acc.append(epoch_loss)\n",
        "  valid_loss.append(epoch_valid_loss)\n",
        "  valid_acc.append(epoch_valid_acc)\n",
        "\n",
        "end = time.time()\n",
        "print(end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qtUCslvwPuu",
        "outputId": "4b0badbb-9693-4fd8-d49d-5ec27b21440f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0 loss:  0.011 accuracy:  0.501 valid_loss:  0.011 valid_accuracy:  0.503\n",
            "epoch:  1 loss:  0.011 accuracy:  0.507 valid_loss:  0.011 valid_accuracy:  0.495\n",
            "epoch:  2 loss:  0.011 accuracy:  0.516 valid_loss:  0.011 valid_accuracy:  0.496\n",
            "epoch:  3 loss:  0.011 accuracy:  0.517 valid_loss:  0.011 valid_accuracy:  0.496\n",
            "epoch:  4 loss:  0.011 accuracy:  0.531 valid_loss:  0.011 valid_accuracy:  0.512\n",
            "131.796147108078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 예측 함수 정의\n",
        "def evaluate(epoch, model, testloader):\n",
        "  test_correct = 0\n",
        "  test_total = 0\n",
        "  test_running_loss = 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for b in testloader:\n",
        "      x, y = b.text, b.label\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_pred = model(x)\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      y_pred = torch.argmax(y_pred, dim=1)\n",
        "      test_correct += (y_pred == y).sum().item()\n",
        "      test_total += y.size(0)\n",
        "      test_running_loss += loss.item()\n",
        "\n",
        "  epoch_test_loss = test_running_loss / len(testloader.dataset)\n",
        "  epoch_test_acc = test_correct / test_total\n",
        "\n",
        "  print('epoch: ', epoch,\n",
        "        'test_loss: ', round(epoch_test_loss,3),\n",
        "        'test_accuracy: ', round(epoch_test_acc,3))\n",
        "  \n",
        "  return epoch_test_loss, epoch_test_acc"
      ],
      "metadata": {
        "id": "fPGkRh6Nw18f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 예측 결과 확인\n",
        "start = time.time()\n",
        "\n",
        "epochs = 5\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_test_loss, epoch_test_acc = evaluate(epoch, model, test_iterator)\n",
        "  test_loss.append(epoch_loss)\n",
        "  test_acc.append(epoch_loss)\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print(end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoHuelCwyUr1",
        "outputId": "d29bb2f4-de15-4412-aca5-ae4e08f8f309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0 test_loss:  0.011 test_accuracy:  0.503\n",
            "epoch:  1 test_loss:  0.011 test_accuracy:  0.503\n",
            "epoch:  2 test_loss:  0.011 test_accuracy:  0.503\n",
            "epoch:  3 test_loss:  0.011 test_accuracy:  0.503\n",
            "epoch:  4 test_loss:  0.011 test_accuracy:  0.503\n",
            "31.64377474784851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN 계층 구현"
      ],
      "metadata": {
        "id": "52AfLKy1zB1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 호출\n",
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time"
      ],
      "metadata": {
        "id": "MbwpBaDsyvJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 내려받기 및 전처리\n",
        "TEXT = torchtext.legacy.data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False, batch_first=True)\n",
        "\n",
        "from torchtext.legacy import datasets\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(split_ratio=0.8)\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=10000, min_freq=10, vectors=None)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "hPbN92YpzvE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 분리\n",
        "train_iterator, valid_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits((train_data, valid_data, test_data),\n",
        "                                                                                            batch_size= BATCH_SIZE,\n",
        "                                                                                            device= device)"
      ],
      "metadata": {
        "id": "SZsbpnqL0Zym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 변수 값 지정\n",
        "vocab_size = len(TEXT.vocab)\n",
        "n_classes = 2"
      ],
      "metadata": {
        "id": "lwl86FL50r3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN 계층 네트워크\n",
        "class BasicRNN(nn.Module):\n",
        "  def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
        "    super(BasicRNN, self).__init__()\n",
        "    self.n_layers = n_layers          # RNN 계층에 대한 개수\n",
        "    self.embed = nn.Embedding(n_vocab, embed_dim) # 워드 임베딩 적용\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.dropout = nn.Dropout(dropout_p)  # 드랍아웃 적용\n",
        "    self.rnn = nn.RNN(embed_dim, self.hidden_dim, num_layers = self.n_layers, batch_first=True)\n",
        "    self.out = nn.Linear(self.hidden_dim, n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embed(x) # 문자를 숫자/벡터로 변환\n",
        "    h_0 = self._init_state(batch_size=x.size(0))  # 최초 은닉 상태의 값을 0으로 초기화\n",
        "    x,_ = self.rnn(x, h_0)  # RNN 계층을 의미하며, 파라미터로 입력과 이전 은닉 상태의 값을 받음\n",
        "    h_t = x[:,-1,:]  # 모든 네트워크를 거쳐서 가장 마지막에 나온 단어의 임베딩 값(마지막 은닉 상태의 값)\n",
        "    self.dropout(h_t)\n",
        "    logit = torch.sigmoid(self.out(h_t))\n",
        "    return logit\n",
        "\n",
        "  def _init_state(self, batch_size=1):\n",
        "    weight = next(self.parameters()).data   # 모델의 파라미터 값을 가져와서 weight 변수에 저장\n",
        "    return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_() \n",
        "    # 크기가 (계층의 개수, 배치 크기, 은닉층의 뉴런/유닛 개수)인 은닉 상태(텐서)를 생성하여 0으로 초기화한 후 반환"
      ],
      "metadata": {
        "id": "f_57iPjT00co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수와 옵티마이저 설정\n",
        "model = BasicRNN(n_layers=2, hidden_dim=256, n_vocab=vocab_size, embed_dim=128, n_classes=n_classes, dropout_p=0.5)\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "Ioru7Btq3vnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 함수\n",
        "def train(model, optimizer, train_iter):\n",
        "  model.train()\n",
        "  for b, batch in enumerate(train_iter):\n",
        "    x, y = batch.text.to(device), batch.label.to(device)\n",
        "    y.data.sub_(1) # 레이블 값을 0과 1로 변환\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logit = model(x)\n",
        "    loss = F.cross_entropy(logit, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if b % 50 == 0:\n",
        "      print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(e,\n",
        "                                                                      b * len(x),\n",
        "                                                                      len(train_iter.dataset),\n",
        "                                                                      100. * b / len(train_iter),\n",
        "                                                                      loss.item()))"
      ],
      "metadata": {
        "id": "kKHc6jJW4xeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가 함수\n",
        "def evaluate(model, val_iter):\n",
        "  model.eval()\n",
        "  corrects, total, total_loss = 0,0,0\n",
        "\n",
        "  for batch in val_iter:\n",
        "    x, y = batch.text.to(device), batch.label.to(device)\n",
        "    y.data.sub_(1)\n",
        "    logit = model(x)\n",
        "    loss = F.cross_entropy(logit, y, reduction='sum')\n",
        "    total += y.size(0)\n",
        "    total_loss += loss.item()\n",
        "    corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
        "\n",
        "  avg_loss = total_loss / len(val_iter.dataset)\n",
        "  avg_accuracy = corrects / total\n",
        "  return avg_loss, avg_accuracy"
      ],
      "metadata": {
        "id": "ndvjP_sN6itx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`(logit.max(1)[1].view(y.size()).data == y.data).sum()`**  \n",
        "\n",
        "1. `max(1)[1]`: .max(dim=0)[0]은 최댓값(max)을 나타내고 .max(dim=0)[1]은 최댓값을 갖는 데이터의 인덱스를 나타냄\n",
        "2. `view(y.size())`: logit.max(1)[1]의 결과를 y.size()로 크기 변경\n",
        "3. `data == y.data`: 모델의 예측 결과(logit.max(1)[1].view(y.size().data)가 레이블(실제 값, y.data)과 같은지 확인\n",
        "4. `sum()`: 모델의 예측 결과와 레이블(실제 값)이 같으면 그 합을 corrects 변수에 누적하여 저장함\n"
      ],
      "metadata": {
        "id": "G1grludx729h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 및 평가\n",
        "BATCH_SiZE = 100\n",
        "LR = 0.001\n",
        "EPOCHS = 5\n",
        "\n",
        "for e in range(1, EPOCHS + 1):\n",
        "  train(model, optimizer, train_iterator)\n",
        "  val_loss, val_accuracy = evaluate(model, valid_iterator)\n",
        "  print('[EPOCH: %d], validation Loss: %5.2f | Validation Accuracy: %5.2f' % (e, val_loss, val_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6NiH5w_7got",
        "outputId": "c85ef91c-ee44-4c40-b8ad-3529629f5801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.695041\n",
            "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.692548\n",
            "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.692676\n",
            "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.698040\n",
            "[EPOCH: 1], validation Loss:  0.69 | Validation Accuracy:  0.50\n",
            "Train Epoch: 2 [0/20000 (0%)]\tLoss: 0.695782\n",
            "Train Epoch: 2 [5000/20000 (25%)]\tLoss: 0.693332\n",
            "Train Epoch: 2 [10000/20000 (50%)]\tLoss: 0.692646\n",
            "Train Epoch: 2 [15000/20000 (75%)]\tLoss: 0.697713\n",
            "[EPOCH: 2], validation Loss:  0.69 | Validation Accuracy:  0.50\n",
            "Train Epoch: 3 [0/20000 (0%)]\tLoss: 0.694739\n",
            "Train Epoch: 3 [5000/20000 (25%)]\tLoss: 0.693001\n",
            "Train Epoch: 3 [10000/20000 (50%)]\tLoss: 0.692967\n",
            "Train Epoch: 3 [15000/20000 (75%)]\tLoss: 0.692863\n",
            "[EPOCH: 3], validation Loss:  0.69 | Validation Accuracy:  0.50\n",
            "Train Epoch: 4 [0/20000 (0%)]\tLoss: 0.690367\n",
            "Train Epoch: 4 [5000/20000 (25%)]\tLoss: 0.693722\n",
            "Train Epoch: 4 [10000/20000 (50%)]\tLoss: 0.693168\n",
            "Train Epoch: 4 [15000/20000 (75%)]\tLoss: 0.694871\n",
            "[EPOCH: 4], validation Loss:  0.70 | Validation Accuracy:  0.50\n",
            "Train Epoch: 5 [0/20000 (0%)]\tLoss: 0.693150\n",
            "Train Epoch: 5 [5000/20000 (25%)]\tLoss: 0.694238\n",
            "Train Epoch: 5 [10000/20000 (50%)]\tLoss: 0.690848\n",
            "Train Epoch: 5 [15000/20000 (75%)]\tLoss: 0.690432\n",
            "[EPOCH: 5], validation Loss:  0.70 | Validation Accuracy:  0.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model, test_iterator)\n",
        "print('Test Loss: %5.2f | Test Accuracy: %5.2f' % (test_loss, test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGr0-NTX89Iw",
        "outputId": "bc446256-e9f6-445a-c423-a8ff296d85e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss:  0.71 | Test Accuracy:  0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 복습"
      ],
      "metadata": {
        "id": "hHf1T7riq_T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time"
      ],
      "metadata": {
        "id": "aP8O5lvm98cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy6d2sepsDrX",
        "outputId": "33898d05-ef0b-455a-a1d0-87e723cc7125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.10.0\n",
            "  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0 torchtext-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = torchtext.legacy.data.Field(lower=True, fix_length=200, batch_first=False)\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False)"
      ],
      "metadata": {
        "id": "IwhToibgrjcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy import datasets\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "metadata": {
        "id": "gKvg84bjr2Wb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908e1bb0-456d-4c16-814b-5a7fc1242ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 9.38MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vars(train_data.examples[0])"
      ],
      "metadata": {
        "id": "2z5eH2o5s4Wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "059b4b3b-735a-432e-db9d-7da752ab663b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'pos',\n",
              " 'text': ['the',\n",
              "  'italian',\n",
              "  'is',\n",
              "  'an',\n",
              "  'astonishingly',\n",
              "  'accomplished',\n",
              "  'film',\n",
              "  'for',\n",
              "  'its',\n",
              "  'time.',\n",
              "  'stunningly',\n",
              "  'shot,',\n",
              "  'with',\n",
              "  'lighting',\n",
              "  'effects',\n",
              "  'that',\n",
              "  'are',\n",
              "  'truly',\n",
              "  'sublime,',\n",
              "  'this',\n",
              "  'is',\n",
              "  'an',\n",
              "  'early',\n",
              "  'gem',\n",
              "  'that',\n",
              "  'clearly',\n",
              "  'reveals',\n",
              "  'reginald',\n",
              "  'barker',\n",
              "  'to',\n",
              "  'be',\n",
              "  'a',\n",
              "  'pioneer',\n",
              "  'director',\n",
              "  'of',\n",
              "  'equal',\n",
              "  'standing',\n",
              "  'to',\n",
              "  'd.w.',\n",
              "  'griffith',\n",
              "  'and',\n",
              "  'maurice',\n",
              "  'tourneur.',\n",
              "  'how',\n",
              "  'much',\n",
              "  'control',\n",
              "  'thomas',\n",
              "  'ince',\n",
              "  'exerted',\n",
              "  'over',\n",
              "  'the',\n",
              "  'production',\n",
              "  'is',\n",
              "  'hard',\n",
              "  'to',\n",
              "  'know,',\n",
              "  'but',\n",
              "  'this',\n",
              "  'film',\n",
              "  'still',\n",
              "  'has',\n",
              "  'extraordinary',\n",
              "  'power.',\n",
              "  'the',\n",
              "  'simple',\n",
              "  'story',\n",
              "  'of',\n",
              "  'an',\n",
              "  'italian',\n",
              "  'immigrant',\n",
              "  'struggling',\n",
              "  'to',\n",
              "  'keep',\n",
              "  'his',\n",
              "  'family',\n",
              "  'alive',\n",
              "  'in',\n",
              "  'new',\n",
              "  'york,',\n",
              "  'is',\n",
              "  'very',\n",
              "  'moving.',\n",
              "  'the',\n",
              "  'themes',\n",
              "  'of',\n",
              "  'social',\n",
              "  'injustice,',\n",
              "  'revenge',\n",
              "  'and',\n",
              "  'forgiveness',\n",
              "  'are',\n",
              "  'completely',\n",
              "  'relevant',\n",
              "  'today.',\n",
              "  'the',\n",
              "  'use',\n",
              "  'of',\n",
              "  'close-ups',\n",
              "  'is',\n",
              "  'outstanding',\n",
              "  'and',\n",
              "  'the',\n",
              "  'powerhouse',\n",
              "  'performance',\n",
              "  'of',\n",
              "  'george',\n",
              "  'beban',\n",
              "  'is',\n",
              "  'electrifying.',\n",
              "  'what',\n",
              "  'we',\n",
              "  'need',\n",
              "  'now',\n",
              "  'is',\n",
              "  'a',\n",
              "  'really',\n",
              "  'good',\n",
              "  'print',\n",
              "  'transferred',\n",
              "  'to',\n",
              "  'dvd',\n",
              "  'so',\n",
              "  'we',\n",
              "  'can',\n",
              "  'truly',\n",
              "  'appreciate',\n",
              "  'this',\n",
              "  'early',\n",
              "  'masterpiece',\n",
              "  'of',\n",
              "  'cinema.']}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "for example in train_data.examples:\n",
        "  text = [x.lower() for x in vars(example)['text']]\n",
        "  text = [x.replace('<br', '') for x in text]\n",
        "  text = [''.join(c for c in s if c not in string.punctuation) for s in text]\n",
        "  text = [s for s in text if s]\n",
        "  vars(example)['text'] = text"
      ],
      "metadata": {
        "id": "dM1vxza9tAPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(0), split_ratio=0.8)"
      ],
      "metadata": {
        "id": "Spwt4p6Yt15s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "metadata": {
        "id": "K7shI40xuRzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train_data, max_size=10000, min_freq=10, vectors=None)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'Unique token in TEXT vocabulary: {len(TEXT.vocab)}')\n",
        "print(f'Unique token in LABEL vocabulary: {len(LABEL.vocab)}')"
      ],
      "metadata": {
        "id": "BiTtcMx5uiL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "embedding_dim = 100\n",
        "hidden_size = 300\n",
        "\n",
        "train_iteration, valid_iteraton, test_iteration = torchtext.legacy.data.BucketIterator.splits(\n",
        "                                                                                                (train_data, valid_data, test_data),\n",
        "                                                                                               batch_size = BATCH_SIZE,\n",
        "                                                                                               device=device )"
      ],
      "metadata": {
        "id": "F48Pg2e7vBzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCell_Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_size):\n",
        "    super(RNNCell_Encoder, self).__init__()\n",
        "    self.rnn = nn.RNNCell(input_dim, hidden_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    bz = inputs.shape[1]\n",
        "    ht = torch.zeros((bz, hidden_size)).to(device)\n",
        "\n",
        "    for word in inputs:\n",
        "      ht = self.rnn(word, ht)\n",
        "    return ht\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.em = nn.Embedding(len(TEXT.vocab.stoi), embedding_dim)\n",
        "    self.rnn = RNNCell_Encoder(embedding_dim, hidden_size)\n",
        "    self.fc1 = nn.Linear(hidden_size, 256)\n",
        "    self.fc2 = nn.Linear(256, 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.em(x)\n",
        "    x = self.rnn(x)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "mD6heTRSwDLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "L_wHYqtey8Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(epoch, model, trainloader, validloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  running_loss = 0\n",
        "\n",
        "  model.train()\n",
        "  for b in trainloader:\n",
        "    x, y = b.text, b.label\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      y_pred = torch.argmax(y_pred, dim=1)\n",
        "      correct += (y_pred == y).sum().item()\n",
        "      total += y.size(0)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "  epoch_loss = running_loss / len(trainloader.dataset)\n",
        "  epoch_acc = correct / total\n",
        "\n",
        "  valid_correct = 0\n",
        "  valid_total = 0\n",
        "  valid_running_loss = 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for b in validloader:\n",
        "      x, y = b.text, b.label\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_pred = model(x)  # 각각의 확률값이 나옴\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      y_pred = torch.argmax(y_pred, dim=1)      # 가장 확률이 큰 인덱스를 추출\n",
        "      valid_correct += (y_pred==y).sum().item() # 실제 레이블과 인덱스를 비교하여 맞는 것들을 합계\n",
        "      valid_total += y.size(0)\n",
        "      valid_running_loss += loss.item()\n",
        "\n",
        "  epoch_valid_loss = valid_running_loss / len(validloader.dataset)\n",
        "  epoch_valid_acc = valid_correct / valid_total\n",
        "\n",
        "  print('epoch: ', epoch,\n",
        "        'loss: ', round(epoch_loss, 3),\n",
        "        'accuracy: ', round(epoch_acc, 3),\n",
        "        'valid_loss: ', round(epoch_valid_loss, 3),\n",
        "        'valid_accuracy: ', round(epoch_valid_acc,3))\n",
        "  \n",
        "  return epoch_loss, epoch_acc, epoch_valid_loss, epoch_valid_acc"
      ],
      "metadata": {
        "id": "t1SsRUmA6ZpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "epoch = 5\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "valid_loss = []\n",
        "valid_acc = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss, epoch_acc, epoch_valid_loss, epoch_valid_acc = training(epoch, model, train_iteration, valid_iteraton)\n",
        "  train_loss.append(epoch_loss)\n",
        "  train_acc.append(epoch_acc)\n",
        "  valid_loss.append(epoch_valid_loss)\n",
        "  valid_acc.append(epoch_valid_acc)\n",
        "\n",
        "end = time.time()\n",
        "print(end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pudrNKV0WbV",
        "outputId": "071b2bab-9cd7-47a0-cf9f-044057c7df86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0 loss:  0.011 accuracy:  0.49 valid_loss:  0.011 valid_accuracy:  0.506\n",
            "epoch:  1 loss:  0.011 accuracy:  0.505 valid_loss:  0.011 valid_accuracy:  0.499\n",
            "epoch:  2 loss:  0.011 accuracy:  0.511 valid_loss:  0.011 valid_accuracy:  0.497\n",
            "epoch:  3 loss:  0.011 accuracy:  0.521 valid_loss:  0.011 valid_accuracy:  0.496\n",
            "epoch:  4 loss:  0.011 accuracy:  0.523 valid_loss:  0.011 valid_accuracy:  0.511\n",
            "92.5865728855133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(epoch, model, testloader):\n",
        "  test_correct = 0\n",
        "  test_total = 0\n",
        "  test_running_loss = 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for b in testloader:\n",
        "      x, y = b.text, b.label\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_pred = model(x)\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      y_pred = torch.argmax(y_pred, dim=1)\n",
        "\n",
        "      test_correct += (y_pred == y).sum().item()\n",
        "      test_total += y.size(0)\n",
        "      test_running_loss += loss.item()\n",
        "\n",
        "  epoch_test_loss = test_running_loss / len(testloader.dataset)\n",
        "  epoch_test_acc = test_correct / test_total\n",
        "\n",
        "  print('epoch: ', epoch,\n",
        "        'test_loss: ', round(epoch_test_loss, 3),\n",
        "        'test_accuracy: ', round(epoch_test_acc,3) )\n",
        "  \n",
        "  return epoch_test_loss, epoch_test_acc"
      ],
      "metadata": {
        "id": "39cK8Ngi0siS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "epochs = 5\n",
        "test_loss = []\n",
        "test_acc = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_test_loss, epoch_test_acc = evaluate(epoch, model, test_iteration)\n",
        "  test_loss.append(epoch_test_loss)\n",
        "  test_acc.append(epoch_test_acc)\n",
        "\n",
        "end = time.time()\n",
        "print(end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-kD28Hv9hjd",
        "outputId": "f605f94d-cb33-4b4b-c1ea-9543be64aa36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0 test_loss:  0.011 test_accuracy:  0.5\n",
            "epoch:  1 test_loss:  0.011 test_accuracy:  0.5\n",
            "epoch:  2 test_loss:  0.011 test_accuracy:  0.5\n",
            "epoch:  3 test_loss:  0.011 test_accuracy:  0.5\n",
            "epoch:  4 test_loss:  0.011 test_accuracy:  0.5\n",
            "32.00366711616516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = torchtext.legacy.data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False, batch_first=True)\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(split_ratio=0.8)\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=10000, min_freq=10, vectors=None)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "8PhVLnDj-lPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iterator, valid_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits((train_data, valid_data, test_data),\n",
        "                                                                                            batch_size=BATCH_SIZE, \n",
        "                                                                                            device = device)"
      ],
      "metadata": {
        "id": "5k9lvRyVBRv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(TEXT.vocab)\n",
        "n_classes = 2"
      ],
      "metadata": {
        "id": "KIYe63nwBlCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicRNN(nn.Module):\n",
        "  def __init__(self, n_layers, hidden_dim, \n",
        "               n_vocab, embed_dim, n_classes, \n",
        "               dropout_p=0.2):\n",
        "    super(BasicRNN, self).__init__()\n",
        "    self.n_layers = n_layers\n",
        "    self.embed = nn.Embedding(n_vocab, embed_dim)\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "    self.rnn = nn.RNN(embed_dim, self.hidden_dim, \n",
        "                      num_layers=self.n_layers, batch_first=True)\n",
        "    self.out = nn.Linear(self.hidden_dim, n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embed(x)\n",
        "    h_0 = self._init_state(batch_size=x.size(0))\n",
        "    x, _ = self.rnn(x, h_0)\n",
        "    h_t = x[:,-1,:]\n",
        "    self.dropout(h_t)\n",
        "    logit = torch.sigmoid(self.out(h_t))\n",
        "    return logit\n",
        "\n",
        "  def _init_state(self, batch_size=1):\n",
        "    weight = next(self.parameters()).data\n",
        "    return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
      ],
      "metadata": {
        "id": "Z8bgxtlwFRar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BasicRNN(n_layers=1, hidden_dim=256, \n",
        "                 n_vocab=vocab_size, embed_dim=128, \n",
        "                 n_classes=n_classes, dropout_p=0.5)\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)"
      ],
      "metadata": {
        "id": "efu2-bD6HXqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, train_iter):\n",
        "  model.train()\n",
        "\n",
        "  for b, batch in enumerate(train_iter):\n",
        "    x, y = batch.text.to(device), batch.label.to(device)\n",
        "    y.data.sub_(1)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logit = model(x)\n",
        "    loss = F.cross_entropy(logit, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if b % 50 == 0:\n",
        "      print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(e,\n",
        "                                                                      b * len(x),\n",
        "                                                                      len(train_iter.dataset),\n",
        "                                                                      100. * b / len(train_iter),\n",
        "                                                                      loss.item()))"
      ],
      "metadata": {
        "id": "Qi3Yp4hWIMge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_iter):\n",
        "  model.eval()\n",
        "  corrects, total, total_loss = 0,0,0\n",
        "\n",
        "  for batch in val_iter:\n",
        "    x, y = batch.text.to(device), batch.label.to(device)\n",
        "    y.data.sub_(1)\n",
        "    logit = model(x)\n",
        "    loss = F.cross_entropy(logit, y, reduction='sum')\n",
        "    total += y.size(0)\n",
        "    total_loss += loss.item()\n",
        "    corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
        "\n",
        "  avg_loss = total_loss / len(val_iter.dataset)\n",
        "  avg_accuracy = corrects / total\n",
        "  \n",
        "  return avg_loss, avg_accuracy"
      ],
      "metadata": {
        "id": "INeq835YJHsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "LR = 0.001\n",
        "EPOCHS = 5\n",
        "\n",
        "for e in range(1, EPOCHS + 1):\n",
        "  train(model, optimizer, train_iterator)\n",
        "  val_loss, val_accuracy = evaluate(model, valid_iterator)\n",
        "  print('[EPOCH: %d], Validation Loss: %5.2f | Validation Accuracy: %5.2f' \n",
        "        % (e, val_loss, val_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLO-Yg0FK2oa",
        "outputId": "cd2c3010-5852-445f-b032-01b8f08a2db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.693775\n",
            "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.692136\n",
            "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.693989\n",
            "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.693454\n",
            "[EPOCH: 1], Validation Loss:  0.69 | Validation Accuracy:  0.50\n",
            "Train Epoch: 2 [0/20000 (0%)]\tLoss: 0.691999\n",
            "Train Epoch: 2 [5000/20000 (25%)]\tLoss: 0.693673\n",
            "Train Epoch: 2 [10000/20000 (50%)]\tLoss: 0.691525\n",
            "Train Epoch: 2 [15000/20000 (75%)]\tLoss: 0.692042\n",
            "[EPOCH: 2], Validation Loss:  0.69 | Validation Accuracy:  0.50\n",
            "Train Epoch: 3 [0/20000 (0%)]\tLoss: 0.694340\n",
            "Train Epoch: 3 [5000/20000 (25%)]\tLoss: 0.692903\n",
            "Train Epoch: 3 [10000/20000 (50%)]\tLoss: 0.695961\n",
            "Train Epoch: 3 [15000/20000 (75%)]\tLoss: 0.690460\n",
            "[EPOCH: 3], Validation Loss:  0.69 | Validation Accuracy:  0.50\n",
            "Train Epoch: 4 [0/20000 (0%)]\tLoss: 0.693822\n",
            "Train Epoch: 4 [5000/20000 (25%)]\tLoss: 0.697055\n",
            "Train Epoch: 4 [10000/20000 (50%)]\tLoss: 0.692585\n",
            "Train Epoch: 4 [15000/20000 (75%)]\tLoss: 0.694017\n",
            "[EPOCH: 4], Validation Loss:  0.69 | Validation Accuracy:  0.49\n",
            "Train Epoch: 5 [0/20000 (0%)]\tLoss: 0.694700\n",
            "Train Epoch: 5 [5000/20000 (25%)]\tLoss: 0.693430\n",
            "Train Epoch: 5 [10000/20000 (50%)]\tLoss: 0.693025\n",
            "Train Epoch: 5 [15000/20000 (75%)]\tLoss: 0.694156\n",
            "[EPOCH: 5], Validation Loss:  0.69 | Validation Accuracy:  0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model, test_iterator)\n",
        "print(\"Test Loss: %5.2f | Test Accuracy: %5.2f\" % (test_loss, test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8ZwtjWjLtsA",
        "outputId": "044bbc11-383f-48b5-96d5-da567e3f8ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss:  0.69 | Test Accuracy:  0.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "TEXT = torchtext.legacy.data.Field(lower= True, fix_length=200, batch_first=False)\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False)"
      ],
      "metadata": {
        "id": "mxexgeFbYXXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy import datasets\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01_gv1L9YzmU",
        "outputId": "301ed458-165b-4489-b54b-a595976c3e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:09<00:00, 8.75MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "for example in train_data.examples:\n",
        "  text = [x.lower() for x in vars(example)['text']]\n",
        "  text = [x.replace(\"<br\", \"\") for x in text]\n",
        "  text = [''.join(c for c in s if c not in string.punctuation) for s in text]\n",
        "  text = [s for s in text if s]\n",
        "  vars(example)['text'] = text"
      ],
      "metadata": {
        "id": "smTqIFZHZRUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(0), \n",
        "                                          split_ratio=0.8)"
      ],
      "metadata": {
        "id": "iKXGq9YYaKmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRqv6axydgFl",
        "outputId": "bf09fd74-0b74-471b-d6b3-30507d549aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 20000\n",
            "Number of validation examples: 5000\n",
            "Number of testing examples: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train_data, max_size=10000, min_freq=10, vectors= None)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'unique tokens in TEXT vocabulary: {len(TEXT.vocab)}')\n",
        "print(f'unique tokens in LABEL vocabulary: {len(LABEL.vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIG9aNlVdzEg",
        "outputId": "bb9cf096-e1f0-4bb3-8fd1-397690af5cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique tokens in TEXT vocabulary: 10002\n",
            "unique tokens in LABEL vocabulary: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_size = 300  # 은닉층의 유닛 개수(D_h)\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device )"
      ],
      "metadata": {
        "id": "J-xddobYeOvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCell_Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_size):\n",
        "    super(RNNCell_Encoder, self).__init__()\n",
        "    self.rnn = nn.RNNCell(input_dim, hidden_size)\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    bz = inputs.shape[1]\n",
        "    ht = torch.zeros((bz, hidden_size)).to(device)\n",
        "\n",
        "    for word in inputs:\n",
        "      ht = self.rnn(word, ht)\n",
        "      \n",
        "    return ht\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.em = nn.Embedding(len(TEXT.vocab.stoi), embeding_dim)\n",
        "    self.rnn = RNNCell_Encoder(embedding_dim, hidden_size)\n",
        "    self.fc1 = nn.Linear(hidden_size, 256)\n",
        "    self.fc2 = nn.Linear(256, 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.em(x)\n",
        "    x = self.rnn(x)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "uYuWxUrIeoNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6X9wyciMG3Y9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}