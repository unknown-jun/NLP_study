{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_word_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPN/uxtKWdUyPW8FSs8buEQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b4438ba021d94aef87189208622d73ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b51207d723834306b2fe81b7d59a05e4",
              "IPY_MODEL_b87838fb1b89427ba1bd95026ae4b6cf",
              "IPY_MODEL_86de5965a1544a1cb0ca311811743abf"
            ],
            "layout": "IPY_MODEL_002a0832eb31426093600cf389c87eb2"
          }
        },
        "b51207d723834306b2fe81b7d59a05e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_077051cd01d34bea959edd5cca0f3455",
            "placeholder": "​",
            "style": "IPY_MODEL_68000c6cff0b4f9aab8c5a194b84626b",
            "value": "Downloading: 100%"
          }
        },
        "b87838fb1b89427ba1bd95026ae4b6cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd9b59be39084803ad717e674cdfe882",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f470216730a648dfbe7157afd85e28c7",
            "value": 440473133
          }
        },
        "86de5965a1544a1cb0ca311811743abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_352f03e2beb44041ac6223b294f5a300",
            "placeholder": "​",
            "style": "IPY_MODEL_f88d478f722f4678a41747c2ef62b060",
            "value": " 420M/420M [00:14&lt;00:00, 30.4MB/s]"
          }
        },
        "002a0832eb31426093600cf389c87eb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "077051cd01d34bea959edd5cca0f3455": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68000c6cff0b4f9aab8c5a194b84626b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd9b59be39084803ad717e674cdfe882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f470216730a648dfbe7157afd85e28c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "352f03e2beb44041ac6223b294f5a300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f88d478f722f4678a41747c2ef62b060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unknown-jun/NLP_study/blob/main/Self_Implement/BERT_word_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT word embedding\n",
        "\n",
        "- [출처](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#33-creating-word-and-sentence-vectors-from-hidden-states)"
      ],
      "metadata": {
        "id": "J6qoTw11s4Jc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-YtsSxlswPC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "h4yF4vxFtcTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Input Formatting\n",
        "BERT는 특정한 구조를 가진 input 데이터를 필요로 하는 모델이기 때문에 다음과 같은 input들이 필요하다.\n",
        "1. A special token `[SEP]`: 문장의 끝을 표시하거나 두 문장 사이를 나누는 역할을 한다.\n",
        "2. A special token `[CLS]`: 문장의 시작 지점에 나타나며 Classification task에 사용되지만 어떠한 task든 간에 있어야 한다.\n",
        "3. BERT의 양식에 맞는 Token\n",
        "4. BERT의 tokenizer으로 부터 나온 Token ID\n",
        "5. Mask ID는 Sequence의 어떤 요소가 토큰인지 padding 요소인지를 나타낸다.\n",
        "6. Segment ID는 다른 문장들을 구분하기 위해 사용된다.\n",
        "7. Positional Embeddings은 문장 내에서 토큰의 위치를 알려주기 위해 사용된다.\n",
        "\n",
        "`transformers`는 이 모든 요소를 쉽게 처리해준다.  \n",
        "그러나 이번 노트북에서는 BERT에 대한 원리를 배우기 위함이므로 이 모든 과정을 직접 구현하게 될 것이다."
      ],
      "metadata": {
        "id": "JQBsWv1lvMd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Special Tokens\n",
        "BERT는 하나 혹은 두개의 문장을 input으로 받아들일수 있으며 special token인 `[SEP]`는 문장들을 구분하는 역할을 한다. `[CLS]` token은 항상 문장 맨 처음에 등장하며 이는 classification task에 쓰인다.\n",
        "\n",
        "그러나 만약 한 개의 문장만을 가지고 있거나, BERT를 classificaion에 쓰지 않더라도 위의 두개의 token은 항상 필요로 한다. BERT는 두 token을 가지고 학습했으므로 사용 시에도 두 token을 받을 것으로 기대하며 작동한다.\n",
        "\n",
        "- 2 Sentence Input:  \n",
        "  `[CLS] The man went to the store. [SEP] He bought a gallon of milk`\n",
        "- 1 Sentence Input:  \n",
        "`[CLS] The man went to the store. [SEP]`"
      ],
      "metadata": {
        "id": "TEATlZjlyaPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Tokenization\n",
        "BERT는 고유한 tokenizer를 가진다. 아래는 한 문장내에서 tokenizer가 작동하는 지에 대한 내용이다."
      ],
      "metadata": {
        "id": "SgeccLDD0G2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Here is the sentence I want embeddings for\"\n",
        "marked_text = '[CLS] ' + text + ' [SEP]'\n",
        "\n",
        "# BERT tokenizer를 이용하여 tokenize\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "tokenized_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRafzzD2u76M",
        "outputId": "47716fbf-712a-46c2-eaff-a2b65a073556"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'here',\n",
              " 'is',\n",
              " 'the',\n",
              " 'sentence',\n",
              " 'i',\n",
              " 'want',\n",
              " 'em',\n",
              " '##bed',\n",
              " '##ding',\n",
              " '##s',\n",
              " 'for',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT의 word embedding은 다음과 같이 단어를 분리하였다.  \n",
        "`['em', '##bed', '##ding', '##s']`\n",
        "\n",
        "원래의 문장은 더 작은 subwords와 문자로 분리된 것을 알 수 있다.subword 앞의 ##는 BERT tokenizer의 방식인데 이는 문자나 subword가 더 큰 단어에 속하는지 다른 subword에 속하는지를 알려주는 기호다.\n",
        "예를 들어 '##bed' token은 'bed' token와는 별개의 token으로의 특성을 가진다. 첫번째 token은 더 큰 단어 안에서 'bed'가 나타날 때 마다 사용되며, 후자는 'bed' 그 자체의 독립적인 뜻을 가진 token으로써 '사람이 자기위해 사용되는 물건'이라는 의미를 가진다.\n",
        "\n",
        "왜 이런 방식으로 BERT tokenizer가 작동하는가? 이는 BERT tokenizer가 WordPiece 모델의 방식을 차용했기 때문이다. 이 모델은 탐욕스럽게(greedily) 언어 데이터에 적합한 독립적인 문자, subwords, 단어를 고정된 크기의 vocabulary로 생성한다. \n",
        "\n",
        "BERT tokenizer 모델의 어휘는 30,000개으로 제한되기 때문에, WordPiece 모델은 모든 영어 character들과 모델이 영어 corpus에서 학습해 온 30,000개의 단어와 subword를 포함하는 단어를 생성한다. 이 vocabulary는 다음과 같은 네 가지 요소를 포함한다.\n",
        "\n",
        "1. 모든 단어\n",
        "2. 단어 앞이나 독립적으로 발생한 Subword(\"embedding\"과 \"go get em\" 안의 \"em\"은 독립형 sequence로써 동일한 벡터로 할당된다.)\n",
        "3. 단어 앞에 속하지 않은 Subword, 이 경우는 앞에 ##를 붙여 구분한다.\n",
        "4. 개별적인 character\n",
        "\n",
        "이 모델에 단어를 tokenize 하기 위해선 tokenizer가 첫번째로 하는 일은 전체 단어가 vocabulary(단어 사전)에 속하는지를 체크하는 것이다. 만약 아니라면 tokenizer는 단어를 vocabulary에 있는 가능한 가장 큰 subword로 쪼개려고 시도하고 이를 실패할 경우 마지막 수단으로 단어를 개별적인 chracter로 만든다. 이로 인하여 항상 단어를 최소한의 개별 문자의 모음으로 나타낼 수 있다.\n",
        "\n",
        "vocabulary 단어로 'OOV'나 'UNK'를 이용하여 모든 단어를 할당하는 방식은 vocabulary에 속하지 않는 단어에 대한 임베딩이 어렵지만 WordPiece 방식은 vocabulary에 속하지 않더라도 이를 subword나 character token으로 임베딩 할 수 있다. 이처럼 \"embedding\"이나 vocabulary에 속하지 않는 단어를 Unknown token에 할당하는 것보다 subword token과 같이 ['em', '##bed', '##ding', '##s']처럼 분할함으로써 더 많은 문맥적 의미를 포함시킬 수 있다. 이러한 방식은 subword 임베딩 벡터를 평균화하여 원래 단어와 비슷한 벡터를 생성할 수 있다.  \n",
        "\n",
        "아래는 BERT tokenizer가 가지고 있는 token들의 예시이다. Token 앞에 ##로 시작하는 것은 독립적인 character거나 subword들이다."
      ],
      "metadata": {
        "id": "IMC-lAgW2O-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(tokenizer.vocab.keys())[5000:5020]"
      ],
      "metadata": {
        "id": "Pl_Gw0KK2L7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f827e2-d2b9-4767-dbb9-b8cef4b4765b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['knight',\n",
              " 'lap',\n",
              " 'survey',\n",
              " 'ma',\n",
              " '##ow',\n",
              " 'noise',\n",
              " 'billy',\n",
              " '##ium',\n",
              " 'shooting',\n",
              " 'guide',\n",
              " 'bedroom',\n",
              " 'priest',\n",
              " 'resistance',\n",
              " 'motor',\n",
              " 'homes',\n",
              " 'sounded',\n",
              " 'giant',\n",
              " '##mer',\n",
              " '150',\n",
              " 'scenes']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "text를 token들로 나눈 후에는 문장을 string의 리스트에서 vocabulary의 리스트로 변환하여야 한다.\n",
        "\n",
        "아래는 다른 의미를 가진 'bank'를 포함한 두개의 문장을 임베딩한 결과이다."
      ],
      "metadata": {
        "id": "3Ku3kd8-EURD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 중의적인 의미를 가진 bank를 포함한 두개의 문장의 정의\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
        "       \"fishing on the Mississippi river bank.\"\n",
        "\n",
        "# special token을 추가한다.\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# 문장을 token으로 분리한다.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# vocabulary의 index를 token string에 맵핑한다.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# 단어와 단어가 가진 index들을 print\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "  print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLly8eOBE7SN",
        "outputId": "a6b1afc6-9edf-4117-f60c-afce8e6833a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]           101\n",
            "after         2,044\n",
            "stealing     11,065\n",
            "money         2,769\n",
            "from          2,013\n",
            "the           1,996\n",
            "bank          2,924\n",
            "vault        11,632\n",
            ",             1,010\n",
            "the           1,996\n",
            "bank          2,924\n",
            "robber       27,307\n",
            "was           2,001\n",
            "seen          2,464\n",
            "fishing       5,645\n",
            "on            2,006\n",
            "the           1,996\n",
            "mississippi   5,900\n",
            "river         2,314\n",
            "bank          2,924\n",
            ".             1,012\n",
            "[SEP]           102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Segment ID\n",
        "BERT는 1과 0을 사용하여 두 문장을 구별하며 이를 통해 문장 쌍을 훈련하고 예측한다. 즉, tokenize할 문장은 반드시 0이나 1로 구분되어져야 한다. 그러나 이번 예제에서는 하나의 문장만을 사용할 것이기 때문에 input으로 1을 사용한다. 만약 두개의 문장을 tokenize하고 싶다면 '[SEP]'를 포함한 첫번째 문장을 0으로, 그 뒤의 문장을 1로 할당해야 한다."
      ],
      "metadata": {
        "id": "3zgc345WGYd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 22개의 token들을 sentence 번호 1로 할당한다.\n",
        "segment_ids = [1] * len(tokenized_text)\n",
        "segment_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIW5pMdyGKqu",
        "outputId": "a247129d-c651-484e-d7f4-c857c4847b75"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Extracting Embeddings"
      ],
      "metadata": {
        "id": "5czsOtcvJDPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Running BERT on our text\n",
        "다음으로는 data를 torch의 tensor로 변환하는 작업이 필요하다.BERT Pytorch는 Python의 list 대신에 torch의 tensor형을 요구한다. 다음과 같은 코드를 통해 문서의 형태를 건들지 않으면서 list형을 변환할 수 있다."
      ],
      "metadata": {
        "id": "QSHqsViOJK6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input값을 Pytorch의 tensor형으로 변환\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensor = torch.tensor([segment_ids])"
      ],
      "metadata": {
        "id": "I_Bpf0iTJ9ga"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`from_pretrained`를 호출함으로써 인터넷에 있는 모델을 호출할 수 있다. `bert-base-uncased`를 로드할 때, logging를 사용하여 모델의 정의에 대하여 확인이 가능하다. 모델은 12개의 layer를 사용한 neural network다. layer와 그의 기능에 대한 내용은 이번 post의 범위를 뛰어넘으므로 이 글에서는 설명하지 않는다.\n",
        "\n",
        "`model.eval()`을 이용하여 모델을 훈련 모드가 아닌 평가 모드로 전환한다. 평가 모드로 전환함에 따라 dropout 규제를 사용하지 않도록 한다."
      ],
      "metadata": {
        "id": "kKwS3j1KL9Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 훈련 모델을 로드한다.\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # 모든 은닉층을 출력할지에 대한 여부\n",
        "                                  )\n",
        "\n",
        "# evaluate 모델로 전환함에 따라 순전파만을 사용하게 된다.\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b4438ba021d94aef87189208622d73ef",
            "b51207d723834306b2fe81b7d59a05e4",
            "b87838fb1b89427ba1bd95026ae4b6cf",
            "86de5965a1544a1cb0ca311811743abf",
            "002a0832eb31426093600cf389c87eb2",
            "077051cd01d34bea959edd5cca0f3455",
            "68000c6cff0b4f9aab8c5a194b84626b",
            "cd9b59be39084803ad717e674cdfe882",
            "f470216730a648dfbe7157afd85e28c7",
            "352f03e2beb44041ac6223b294f5a300",
            "f88d478f722f4678a41747c2ef62b060"
          ]
        },
        "id": "4QaynaWxKTIQ",
        "outputId": "0b4d0e88-548e-4823-ea0e-0d9407db2839"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4438ba021d94aef87189208622d73ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음으로, 우리의 예제 text를 BERT를 이용하여 평가하고 network의 은닉층을 호출해보자  \n",
        "*Side Note: 아래의 `torch.no_grad()`은 계산 그래프를 구축하지 않음을 의미한다(여기선 역전파를 사용하지 않기 때문에). 이로 인하여 메모리 소모를 줄일 수 있으며 속도를 약간 향상 시킬 수 있다.*"
      ],
      "metadata": {
        "id": "Nmt8JsPGOUCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text를 BERT에 입력한다.\n",
        "# 그리고 12개의 레이어의 은닉층의 정보를 수집한다.\n",
        "with torch.no_grad():\n",
        "\n",
        "  outputs = model(tokens_tensor, segments_tensor)\n",
        "  hidden_states = outputs[2]  # outputs에 'last_hidden_state', 'pooler_outputs', 'hidden_states'를 포함하고 있다."
      ],
      "metadata": {
        "id": "McBXJ2hkODbk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Understaning the Output\n",
        "앞서 hidden_states 객체에 저장한 BERT에서 나온 모든 은닉층은 이해하기 어려워 보인다. 이 객체는 아래의 순서대로 4개의 차원으로 구성되어 있다.\n",
        "\n",
        "1. 총 layer의 수(13 layers)\n",
        "2. 총 batch의 수(1 sentence)\n",
        "3. 단어나 토큰의 수(예제에서는 22 tokens)\n",
        "4. 은닉층의 unit 수/ feature의 수(768 feature)\n",
        "\n",
        "우리는 앞서 BERT 모델이 총 12개의 레이어로 구성되어 있다고 말했다. 하지만 여기서는 13개의 레이어로 구성되어 있다고 설명하고 있다. 이는 첫번째 레이어는 input embedding layer이기 때문이다. \n",
        "\n",
        "단 한개의 문장을 표현하기 위해 사용된 unique 값은 총 219648개이다 (13 x 1 x 22 x 768)\n",
        " \n",
        "\n",
        "두번째 차원인 batch의 size는 한번에 여러 문장을 모델에 입력할 때 사용된다. 이번 예제에서는 단 하나의 문장이 쓰였다.(batch_size= 1)"
      ],
      "metadata": {
        "id": "wQFFULukQF15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of layers: \", len(hidden_states), \" (initial embeddings + 12 BERT layers)\")\n",
        "layer_i = 0\n",
        "\n",
        "print(\"Number of batches: \", len(hidden_states[layer_i]))\n",
        "batch_i = 0\n",
        "\n",
        "print(\"Number of tokens: \", len(hidden_states[layer_i][batch_i]))\n",
        "token_i = 0\n",
        "\n",
        "print('Number of hidden units: ', len(hidden_states[layer_i][batch_i][token_i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX8pdIK9PxZ7",
        "outputId": "01e28a18-0edc-4ea5-f634-40a901ce4128"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers:  13  (initial embeddings + 12 BERT layers)\n",
            "Number of batches:  1\n",
            "Number of tokens:  22\n",
            "Number of hidden units:  768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "주어진 레이어와 토큰에 대한 값의 범위를 살펴봤을 때, 몇몇을 제외하고는 모든 레이어와 토큰에 대해 작은 값(-2~2)을 분포를 가지고 있음을 알 수 있다."
      ],
      "metadata": {
        "id": "nCehcVIiZy-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제의 문장에서의 5번째 토큰을 살펴보기 위해 \n",
        "# layer 5를 선택하여 그것에 대한 feature들을 가져옴\n",
        "token_i = 5\n",
        "layer_i = 5\n",
        "vec = hidden_states[layer_i][batch_i][token_i]\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.hist(vec, bins=200);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "IgwecFYPWDUC",
        "outputId": "72561749-0ca4-408b-842f-27ea672c8a2d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "layer별로 값을 그룹핑 하는 것도 좋지만, 이 글에서의 목적인 word_embedding을 위하여 token별로 그룹핑하기로 하자\n",
        "\n",
        "- 현재 차원:  \n",
        "`[# layers, # batches, # tokens, # features]`\n",
        "\n",
        "- 목적에 맞게 변형할 차원:  \n",
        "`[# tokens, # layers, #features]`\n",
        "\n",
        "Pytorch의 `permute`함수를 사용하면 tensor의 차원을 쉽게 변형할 수 있다.  \n",
        "그러나 현재 데이터 타입은 tensor가 아니다."
      ],
      "metadata": {
        "id": "mob1kzWwa1T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hidden_state는 자체는 Python의 list\n",
        "print('       Type of hidden_sates: ', type(hidden_states))\n",
        "\n",
        "# 각 layer 안은 torch의 tensor 형태임\n",
        "print('Tensor shape for each layer: ', hidden_states[0].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuJn3s-Tarc5",
        "outputId": "561bc900-1a29-4ede-aec0-5ba83fb557dd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Type of hidden_sates:  <class 'tuple'>\n",
            "Tensor shape for each layer:  torch.Size([1, 22, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "하나의 tensor로 만들기 위해 layer들을 합친다."
      ],
      "metadata": {
        "id": "uwI_f2YJcXsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 layer들을 합치기 위해 'stack'을 사용한다.\n",
        "# stack은 하나의 새로운 차원의 tensor를 생성한다.\n",
        "token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGjWlzcEcfMS",
        "outputId": "5f404e9a-e731-4422-aed2-edaaf50d74b0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 1, 22, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "필요하지 않은 batch 차원을 제거한다."
      ],
      "metadata": {
        "id": "82YXoZ5pc5Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHbxMFa5c2Y3",
        "outputId": "fbd984b8-8d3b-4b68-cb23-f91aebf02b0a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 22, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`permute`을 사용하여 텐서의 차원을 쉽게 재배열 할 수 있다.  \n",
        "이를 이용하여 'layers'와 'tokens'의 순서를 바꾼다."
      ],
      "metadata": {
        "id": "DcIXgeqSdc9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 차원 0과 차원 1을 바꾼다.\n",
        "token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AHzF81gdFR3",
        "outputId": "8a3ce0c1-d2e5-4a3d-9b33-001b232aef14"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([22, 13, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Creating word and sentence vectors from hidden states\n",
        "이제, 은닉층을 가지고 무엇을 할 수 있을까? 우리는 각 token에 대한 개별 vector나 전체 문장의 단일 vector 표현을 얻고 싶지만, 입력의 각 토큰에 대해 각각 768 크기의 13개의 개별 벡터가 존재한다.\n",
        "\n",
        "개별 벡터를 얻기 위해서 몇몇 layer 벡터들을 합쳐야 한다. 그러면 어떤 layer 혹은 layer의 조합이 최상의 결과를 낼 수 있을까?\n",
        "\n",
        "아쉽게도 그에 대한 답은 없다. 하지만 몇가지 합리적인 접근 방식이 있다. 아래는 그에 대한 몇가지 유용한 방법이다."
      ],
      "metadata": {
        "id": "jPoAYR4zfme8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Word vectors\n",
        "두가지 방법으로 단어벡터를 만들어 보자  \n",
        "먼저 마지막 4개의 layer를 연결하여(concatenate) 토큰 당 단일 벡터를 제공한다. 각 벡터의 길이는 `4 x 768 = 3,072`다."
      ],
      "metadata": {
        "id": "jmyXnv92q5dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [22 x 3,072]의 형태를 가진 token vector를 저장한다.\n",
        "token_vecs_cat = []\n",
        "\n",
        "# token_embedding은 [22 x 13 x 768]의 tensor이다.\n",
        "\n",
        "# token_embeddings의 token 레이어에 대해\n",
        "for token in token_embeddings:\n",
        "\n",
        "  # token 레이어는 [12 x 768] tensor다.\n",
        "  # 마지막 4개의 레이어에서 vector들을 꺼내 concatenate한다.\n",
        "  # 각 layer vector는 768 벡터다. 그렇기 때문에 'cat_vec'의 길이는 3,072다.\n",
        "\n",
        "  cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "\n",
        "  token_vecs_cat.append(cat_vec)\n",
        "\n",
        "print('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEzcMKj4h4z4",
        "outputId": "9eb92859-179b-471e-e08e-d6f82298a981"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape is: 22 x 3072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다른 방법으로는 마지막 4개의 layer들을 합쳐(summing) 단어 벡터를 만드는 것이다."
      ],
      "metadata": {
        "id": "ZXfoafM-nM89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [22 x 768]의 shape을 가진 token 벡터를 저장하기 위한 초기 객체\n",
        "token_vecs_sum = []\n",
        "\n",
        "# token_embeddings는 [22 x 12 x 768]의 tensor\n",
        "\n",
        "# token_embeddings의 token 레이어에 대해\n",
        "for token in token_embeddings:\n",
        "\n",
        "  # 마지막 4개의 layer를 합친다.(sum)\n",
        "  sum_vec = torch.sum(token[-4:], dim=0)\n",
        "  token_vecs_sum.append(sum_vec)\n",
        "\n",
        "print('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDfztqhNffrl",
        "outputId": "d8f15c98-6090-464f-8a9a-9ff27fa05806"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape is: 22 x 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence vectors\n",
        "\n",
        "전체 문장에 대한 단일 벡터를 얻기위한 여러 접근 방식이 있지만 가장 간단한 방법은 단일 768크기의 벡터를 생성하는 각 토큰의 두번째에서 마지막 은닉층 레이어를 평균내는 것이다."
      ],
      "metadata": {
        "id": "OuDxbGXsq7LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hidden_states는 [13 x 1 x 22 x 768]의 shape\n",
        "\n",
        "# token_vecs는 [22 x 768]의 shape을 가진다.\n",
        "token_vecs = hidden_states[-2][0]\n",
        "\n",
        "# 모든 22개의 token 벡터들의 평균을 계산한다.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "\n",
        "print(\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pFJf9YTmBzk",
        "outputId": "f70cdd56-a495-4af4-e9b0-9b2bb12ed56a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our final sentence embedding vector of shape: torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Confirming contextually dependent vectors\n",
        "실제로 이러한 벡터들이 문맥적으로 독립적인 값을 가지는 지 확인하기 위하여 예시 문장에서 각 'bank'에 대한 차이를 살펴보기로 하자\n",
        "\n",
        "“After stealing money from the **bank vault**, the **bank robber** was seen fishing on the Mississippi **river bank**.”\n",
        "\n",
        "예제 문장에서 \"bank\"이라는 단어의 세 가지 예시의 index를 찾아보자."
      ],
      "metadata": {
        "id": "2tcAbotCsO5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, token_str in enumerate(tokenized_text):\n",
        "  print(i, token_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4UrM_5-sKwP",
        "outputId": "a2e584ab-cd90-45f2-cd56-14c1dcf7d6da"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [CLS]\n",
            "1 after\n",
            "2 stealing\n",
            "3 money\n",
            "4 from\n",
            "5 the\n",
            "6 bank\n",
            "7 vault\n",
            "8 ,\n",
            "9 the\n",
            "10 bank\n",
            "11 robber\n",
            "12 was\n",
            "13 seen\n",
            "14 fishing\n",
            "15 on\n",
            "16 the\n",
            "17 mississippi\n",
            "18 river\n",
            "19 bank\n",
            "20 .\n",
            "21 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "분석을 위해 마지막 4개의 layer를 합친 단어 벡터를 사용해보자.  \n",
        "이 벡터를 출력하여 다음과 같이 서로 비교해볼 수 있다."
      ],
      "metadata": {
        "id": "lqM8tfb8uXvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('First 5 vectors values for each instance of \"bank\".\\n')\n",
        "print('bank vault   ', str(token_vecs_sum[6][:5]))\n",
        "print('bank robber  ', str(token_vecs_sum[10][:5]))\n",
        "print('river bank   ', str(token_vecs_sum[19][:5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED0SBAmgt1gR",
        "outputId": "5af839d3-8991-4141-9005-a7e134233aaa"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 vectors values for each instance of \"bank\".\n",
            "\n",
            "bank vault    tensor([ 3.3596, -2.9805, -1.5421,  0.7065,  2.0031])\n",
            "bank robber   tensor([ 2.7359, -2.5577, -1.3094,  0.6797,  1.6633])\n",
            "river bank    tensor([ 1.5266, -0.8895, -0.5152, -0.9298,  2.8334])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# 단어 bank 사이의 cosine similarity를 계산한다.\n",
        "# 'bank robber' vs 'river bank' (다른 의미)\n",
        "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
        "\n",
        "# \"bank robber\" vs \"bank vault\"  (같은 의미)\n",
        "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
        "\n",
        "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
        "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45J4kD_5vMx_",
        "outputId": "ab506623-87a2-4446-d20b-bcf116d869c2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector similarity for  *similar*  meanings:  0.94\n",
            "Vector similarity for *different* meanings:  0.69\n"
          ]
        }
      ]
    }
  ]
}